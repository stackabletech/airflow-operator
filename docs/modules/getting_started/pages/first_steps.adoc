= First steps

After going through the xref:installation.adoc[] section and having installed all the Operators, you will now deploy a Airflow cluster and its dependencies. Afterwards you can <<_verify_that_it_works, verify that it works>> by running and tracking an example DAG.

== Setup

As we have installed the external dependencies required by Airflow (Postgresql and Redis) we can now install the Airflow cluster itself.

== Secret with Airflow credentials

A secret with the necessary credentials must be created, this entails database connection credentials as well as an admin account for Airflow itself. Create a file called `airflow-credentials.yaml`:

[source,yaml]
include::example$code/airflow-credentials.yaml[]

And apply it:

[source,bash]
include::example$code/getting-started.sh[tag=apply-airflow-credentials]

The `connections.secretKey` will be used for securely signing the session cookies and can be used
for any other security related needs by extensions. It should be a long random string of bytes.

`connections.sqlalchemyDatabaseUri` must contain the connection string to the SQL database storing
the Airflow metadata.

`connections.celeryResultBackend` must contain the connection string to the SQL database storing the results of Airflow jobs.

`connections.celeryBrokerUrl` must contain the connection string to the broker used for queuing Airflow jobs.

The `adminUser` fields are used to create an admin user.
Please note that the admin user will be disabled if you use a non-default authentication mechanism like LDAP.

=== Airflow

Create a file named `airflow.yaml` with the following contents:

[source,yaml]
----
include::example$code/airflow.yaml[]
----

And apply it:

----
include::example$code/getting-started.sh[tag=install-airflow]
----

`metadata.name` contains the name of the Airflow cluster.

The label of the Docker image provided by Stackable must be set in `spec.version`.

Please note that the version you need to specify is not only the version of Apache Airflow which you want to roll out, but has to be amended with a Stackable version as shown. This Stackable version is the version of the underlying container image which is used to execute the processes. For a list of available versions please check our
https://repo.stackable.tech/#browse/browse:docker:v2%2Fstackable%airflow%2Ftags[image registry].
It should generally be safe to simply use the latest image version that is available.

`spec.statsdExporterVersion` must contain the tag of a statsd-exporter Docker image in the Stackable repository.

The previously created secret must be referenced in `spec.credentialsSecret`.

The `spec.loadExamples` key is optional and defaults to `false`. It is set to `true` here as the example DAGs will be used when verifying the installation.

The `spec.exposeConfig` key is optional and defaults to `false`. It is set to `true` only as an aid to verify the configuration and should never be used as such in anything other than test or demo clusters.

This will create the actual Airflow cluster.

=== Initialization of the Airflow database

When creating an Airflow cluster, a database-initialization job is first started to ensure that the database schema is present and correct (i.e. populated with an admin user). A Kubernetes job is created which starts a pod to initialize the database. This can take a while.

You can use kubectl to wait on the resource, although the cluster itself will not be created until this step is complete.:

[source,bash]
include::example$code/getting-started.sh[tag=wait-airflowdb]

The job status can be inspected and verified like this:

[source,bash]
----
kubectl get jobs
----

which will show something like this:

----
NAME      COMPLETIONS   DURATION   AGE
airflow   1/1           85s        11m
----

Then, make sure that all the Pods in the StatefulSets are ready:

[source,bash]
----
kubectl get statefulset
----

The output should show all pods ready, including the external dependencies:

----
NAME                        READY   AGE
airflow-postgresql          1/1     16m
airflow-redis-master        1/1     16m
airflow-redis-replicas      1/1     16m
airflow-scheduler-default   1/1     11m
airflow-webserver-default   1/1     11m
airflow-worker-default      2/2     11m
----

Then, create a port-forward for the Airflow webserver:

----
include::example$code/getting-started.sh[tag=port-forwarding]
----

== Verify that it works

The Webserver UI can now be opened in the browser with `http://localhost:8080`. Enter the admin credentials from the Kubernetes secret:

image::airflow_login.png[Airflow login screen]

Since the examples were loaded in the cluster definition, they will appear under the DAGs tabs:

image::airflow_dags.png[Example Airflow DAGs]

Select one of these DAGs by clicking on the name in the left-hand column e.g. `example_complex`. Click on the arrow in the top right of the screen, select "Trigger DAG" and the DAG nodes will be automatically highlighted as the job works through its phases.

image::airflow_running.png[Airflow DAG in action]