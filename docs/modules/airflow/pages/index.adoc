= Stackable Operator for Apache Airflow
:description: The Stackable Operator for Apache Airflow is a Kubernetes operator that can manage Apache Airflow clusters. Learn about its features, resources, dependencies and demos, and see the list of supported Airflow versions.
:keywords: Stackable Operator, Apache Airflow, Kubernetes, k8s, operator, engineer, big data, metadata, job pipeline, scheduler, workflow, ETL

:k8s-crs: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/

The Stackable Operator for Apache Airflow manages https://airflow.apache.org/[Apache Airflow] instances on Kubernetes.
Apache Airflow is an open-source application for creating, scheduling, and monitoring workflows. Workflows are defined
as code, with tasks that can be run on a variety of platforms, including Hadoop, Spark, and Kubernetes itself. Airflow
is a popular choice to orchestrate ETL workflows and data pipelines.

== Getting started

Get started using Airflow with the Stackable Operator by following the xref:getting_started/index.adoc[] guide. It
guides you through installing the Operator alongside a PostgreSQL database and Redis instance, connecting to your
Airflow instance and running your first workflow.

=== Custom resources

The AirflowCluster is the resource for the configuration of the Airflow instance. The resource defines three xref:concepts:roles-and-role-groups.adoc[roles]: `webserver`, `worker` and `scheduler` (the `worker` role is embedded within `spec.celeryExecutors`: this is described in the next section). The various configuration options are explained in the xref:usage-guide/index.adoc[]. It helps you tune your cluster to your needs by configuring xref:usage-guide/storage-resources.adoc[resource usage], xref:usage-guide/security.adoc[security], xref:usage-guide/logging.adoc[logging] and more.

=== Executors

The `worker` role is deployed when `spec.celeryExecutors` is specified (the alternative is `spec.kubernetesExecutors`, whereby pods are created dynamically as needed without jobs being routed through a redis queue to the workers). This means that for `kubernetesExecutors` there exists an implicit single role which does not appear in resource definition. This is illustrated below:

==== `celeryExecutors`

[source,yaml]
----
spec:
...
celeryExecutors:
  roleGroups:
    default:
      envOverrides:
        ...
      configOverrides:
        ...
      replicas: 2
  config:
    logging:
          ...
----

==== `kubernetesExecutors`

[source,yaml]
----
spec:
...
kubernetesExecutors:
  config:
    logging:
      ...
    resources:
      ...
----

=== Kubernetes resources

Based on the custom resources you define, the Operator creates ConfigMaps, StatefulSets and Services.

image::airflow_overview.drawio.svg[A diagram depicting the Kubernetes resources created by the operator]

The diagram above depicts all the Kubernetes resources created by the operator, and how they relate to each other.

For every xref:concepts:roles-and-role-groups.adoc#_role_groups[role group] you define, the Operator creates a
StatefulSet with the amount of replicas defined in the RoleGroup. Every Pod in the StatefulSet has two containers: the
main container running Airflow and a sidecar container gathering metrics for xref:operators:monitoring.adoc[]. The
Operator creates a Service per role group as well as a single service for the whole `webserver` role called
`<clustername>-webserver`.

Additionally, a ConfigMap is created for each RoleGroup. These ConfigMaps contain two files, `log_config.py` and `webserver_config.py`, which contain logging and general Airflow configuration respectively.

== Required external components

Airflow requires an SQL database in which to store its metadata as well as Redis for job execution. The xref:required-external-components.adoc[required external components] page lists all supported databases and Redis versions to use in production. You need to provide these components for production use, but the xref:getting_started/index.adoc[] guides you through installing an example database and Redis instance with an Airflow instance that you can use to get started.

NOTE: Redis is only needed if the executors have been set to `spec.celeryExecutors` as the jobs will be queued via Redis before being assigned to a `worker` pod. When using `spec.kubernetesExecutors` the scheduler will take direct responsibility for this.

== Using custom workflows/DAGs

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html[Direct acyclic graphs (DAGs) of tasks] are
the core entities you will use in Airflow. Have a look at the page on xref:usage-guide/mounting-dags.adoc[] to learn
about the different ways of loading your custom DAGs into Airflow.

== Demo

You can install the xref:demos:airflow-scheduled-job.adoc[] demo and explore an Airflow installation, as
well as how it interacts with xref:spark-k8s:index.adoc[Apache Spark].

== Supported Versions

The Stackable Operator for Apache Airflow currently supports the following versions of Airflow:

include::partial$supported-versions.adoc[]
