= Applying Custom Resources
:description: Learn to apply custom resources in Airflow, such as Spark jobs, using Kubernetes connections, roles, and modular DAGs with git-sync integration.

Airflow can be used to apply custom resources from within a cluster.
An example of this could be a SparkApplication job that is to be triggered by Airflow.
The steps below describe how this can be done.
The DAG consists of modularized Python files and is provisioned using the git-sync facility.

== Define an in-cluster Kubernetes connection

To start a Spark job, Airflow needs to be able to communicate with Kubernetes and an in-cluster connection is required for this, which can be created from within the Webserver UI (note that the "in cluster configuration" box is ticked):

image::airflow_connection_ui.png[Airflow Connections]

Alternatively, the connection can be https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html[defined] by an environment variable in URI format:

[source]
AIRFLOW_CONN_KUBERNETES_IN_CLUSTER: "kubernetes://?__extra__=%7B%22extra__kubernetes__in_cluster%22%3A+true%2C+%22extra__kubernetes__kube_config%22%3A+%22%22%2C+%22extra__kubernetes__kube_config_path%22%3A+%22%22%2C+%22extra__kubernetes__namespace%22%3A+%22%22%7D"

This can be supplied directly in the custom resource for all roles (Airflow expects configuration to be common across components):

[source,yaml]
----
include::example$example-airflow-incluster.yaml[]
----

== Define a cluster role for Airflow to create SparkApplication resources

Airflow cannot create or access SparkApplication resources by default - a cluster role is required for this:

[source,yaml]
----
include::example$example-airflow-spark-clusterrole.yaml[]
----

and a corresponding cluster role binding:

[source,yaml]
----
include::example$example-airflow-spark-clusterrolebinding.yaml[]
----

== DAG code

Now for the DAG itself.
The job to be started is a modularized DAG that uses starts a one-off Spark job that calculates the value of pi.
The file structure fetched to the root git-sync folder looks like this:

----
dags
|_ stackable
  |_ __init__.py
  |_ spark_kubernetes_operator.py
  |_ spark_kubernetes_sensor.py
|_ pyspark_pi.py
|_ pyspark_pi.yaml
----

The Spark job calculates the value of pi using one of the example scripts that comes bundled with Spark:

[source,yaml]
----
include::example$example-pyspark-pi.yaml[]
----

This is called from within a DAG by using the connection that was defined earlier.
It is wrapped by the `KubernetesHook` that the Airflow Kubernetes provider makes available https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py[here].
There are two classes that are used to:

* start the job
* monitor the status of the job

The classes `SparkKubernetesOperator` and `SparkKubernetesSensor` are located in two different Python modules as they are typically used for all custom resources and thus are best decoupled from the DAG that calls them.
This also demonstrates that modularized DAGs can be used for Airflow jobs as long as all dependencies exist in or below the root folder pulled by git-sync.

[source,python]
----
include::example$example_spark_kubernetes_operator.py[]
----

[source,python]
----
include::example$example_spark_kubernetes_sensor.py[]
----

[source,python]
----
include::example$example-spark-dag.py[]
----
<1> the wrapper class used for calling the job via `KubernetesHook`
<2> the connection that created for in-cluster usage
<3> the wrapper class used for monitoring the job via `KubernetesHook`
<4> the start of the DAG code
<5> the initial task to invoke the job
<6> the subsequent task to monitor the job
<7> the jobs are chained together in the correct order

Once this DAG is xref:usage-guide/mounting-dags.adoc[mounted] in the DAG folder it can be called and its progress viewed from within the Webserver UI:

image::airflow_dag_graph.png[Airflow Connections]

Clicking on the "spark_pi_monitor" task and selecting the logs shows that the status of the job has been tracked by Airflow:

image::airflow_dag_log.png[Airflow Connections]

NOTE: If the `KubernetesExecutor` is employed the logs are only accessible via the SDP logging mechanism, described https://docs.stackable.tech/home/stable/concepts/logging[here].

TIP: A full example of the above is used as an integration test https://github.com/stackabletech/airflow-operator/tree/main/tests/templates/kuttl/mount-dags-gitsync[here].

== Logging

As mentioned above, the logs are available from the webserver UI if the jobs run with the `celeryExecutor`.
If the SDP logging mechanism has been deployed, log information can also be retrieved from the vector backend (e.g. Opensearch):

image::airflow_dag_log_opensearch.png[Opensearch]
