= Applying Custom Resources
:description: Learn to apply custom resources in Airflow, such as Spark jobs, using Kubernetes connections, roles, and modular DAGs with git-sync integration.
:airflow-managing-connections: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html

Airflow can apply custom resources from within a cluster, such as triggering Spark job by applying a SparkApplication resource.
The steps below outline this process.
The DAG consists of modularized Python files and is provisioned using the git-sync feature.

== Define an in-cluster Kubernetes connection

To start a Spark job, Airflow must communicate with Kubernetes, requiring an in-cluster connection.
This can be created through the Webserver UI by enabling the "in cluster configuration" setting:

image::airflow_connection_ui.png[A screenshot of the 'Edit connection' window with the 'in cluster configuration' tick box ticked]

Alternatively, the connection can be {airflow-managing-connections}[defined] using an environment variable in URI format:

[source]
AIRFLOW_CONN_KUBERNETES_IN_CLUSTER: "kubernetes://?__extra__=%7B%22extra__kubernetes__in_cluster%22%3A+true%2C+%22extra__kubernetes__kube_config%22%3A+%22%22%2C+%22extra__kubernetes__kube_config_path%22%3A+%22%22%2C+%22extra__kubernetes__namespace%22%3A+%22%22%7D"

This can be supplied directly in the custom resource for all roles (Airflow expects configuration to be common across components):

[source,yaml]
----
include::example$example-airflow-incluster.yaml[]
----

== Define a cluster role for Airflow to create SparkApplication resources

Airflow cannot create or access SparkApplication resources by default - a cluster role is required for this:

[source,yaml]
----
include::example$example-airflow-spark-clusterrole.yaml[]
----

and a corresponding cluster role binding:

[source,yaml]
----
include::example$example-airflow-spark-clusterrolebinding.yaml[]
----

== DAG code

For the DAG itself, the job is a modularized DAG that starts a one-off Spark job to calculate the value of pi.
The file structure, fetched to the root git-sync folder, looks like this:

----
dags
|_ stackable
  |_ __init__.py
  |_ spark_kubernetes_operator.py
  |_ spark_kubernetes_sensor.py
|_ pyspark_pi.py
|_ pyspark_pi.yaml
----

The Spark job calculates the value of pi using one of the example scripts that comes bundled with Spark:

[source,yaml]
----
include::example$example-pyspark-pi.yaml[]
----

This is called from within a DAG by using the connection that was defined earlier.
It is wrapped by the `KubernetesHook` that the Airflow Kubernetes provider makes available https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py[here].
There are two classes that are used to:

* start the job
* monitor the status of the job

The classes `SparkKubernetesOperator` and `SparkKubernetesSensor` are located in two different Python modules as they are typically used for all custom resources and thus are best decoupled from the DAG that calls them.
This also demonstrates that modularized DAGs can be used for Airflow jobs as long as all dependencies exist in or below the root folder pulled by git-sync.

[source,python]
----
include::example$example_spark_kubernetes_operator.py[]
----

[source,python]
----
include::example$example_spark_kubernetes_sensor.py[]
----

[source,python]
----
include::example$example-spark-dag.py[]
----
<1> the wrapper class used for calling the job via `KubernetesHook`
<2> the connection that created for in-cluster usage
<3> the wrapper class used for monitoring the job via `KubernetesHook`
<4> the start of the DAG code
<5> the initial task to invoke the job
<6> the subsequent task to monitor the job
<7> the jobs are chained together in the correct order

Once this DAG is xref:usage-guide/mounting-dags.adoc[mounted] in the DAG folder it can be called and its progress viewed from within the Webserver UI:

image::airflow_dag_graph.png[Airflow Connections]

Clicking on the "spark_pi_monitor" task and selecting the logs shows that the status of the job has been tracked by Airflow:

image::airflow_dag_log.png[Airflow Connections]

NOTE: If the `KubernetesExecutor` is employed the logs are only accessible via the SDP logging mechanism, described https://docs.stackable.tech/home/stable/concepts/logging[here].

TIP: A full example of the above is used as an integration test https://github.com/stackabletech/airflow-operator/tree/main/tests/templates/kuttl/mount-dags-gitsync[here].

== Logging

As mentioned above, the logs are available from the webserver UI if the jobs run with the `celeryExecutor`.
If the SDP logging mechanism has been deployed, log information can also be retrieved from the vector backend (e.g. Opensearch):

image::airflow_dag_log_opensearch.png[Opensearch]
