= Using Kubernetes executors
:description: Configure Kubernetes executors in Airflow to dynamically create pods for tasks, replacing Celery executors and bypassing Redis for job routing.

Instead of using the Celery workers you can let Airflow run the tasks using Kubernetes executors, where Pods are created dynamically as needed without jobs being routed through a Redis queue to the workers.

== Kubernetes Executor configuration

To achieve this, swap `spec.celeryExecutors` with `spec.kubernetesExecutors`.
E.g. you would change the following example

[source,yaml]
----
spec:
  celeryExecutors:
    roleGroups:
      default:
        replicas: 2
    config:
      resources:
        # ...
----

to

[source,yaml]
----
spec:
  kubernetesExecutors:
    config:
      resources:
        # ...
----

== Logging

Kubernetes Executors and their respective Pods only live as long as the task they are executing.
Afterwards the Pod is immediately terminated and e.g. console output or logs are gone.

In order to persist executor logs, Airflow can be configured to store its https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/kubernetes_executor.html[executor logs on disk (PV)] or as described in the following section on S3.

=== Airflow Web UI

In the Airflow Web UI, click on `Admin` -> `Connections` -> `Add a new record` (the plus). Then enter your minio host and credentials as shown.

image::airflow_edit_s3_connection.png[Airflow connection menu]

The name or connection ID is `minio`, the type is `Amazon Web Services`, the `AWS Access Key ID` and `AWS Secret Access Key` are filled with the S3 credentials.
The `Extra` field contains the endpoint URL like:

[source,json]
----
{
  "endpoint_url": "http://minio.minio.svc.cluster.local:9000"
}
----

=== Executor configuration

Now we can add ENV variables to the Airflow cluster definition and configure the S3 logging.

[source,yaml]
----
include::example$example-airflow-kubernetes-executor-s3-logging.yaml[]
----

Now you should be able to fetch and inspect logs in the Airflow Web UI from S3 for each DAG.

image::airflow_dag_s3_logs.png[Airflow DAG S3 logs]
