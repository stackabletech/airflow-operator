apiVersion: kuttl.dev/v1beta1
kind: TestStep
metadata:
  name: install-airflow-db
timeout: 480
---
apiVersion: v1
kind: Secret
metadata:
  name: test-airflow-credentials
type: Opaque
stringData:
  adminUser.username: airflow
  adminUser.firstname: Airflow
  adminUser.lastname: Admin
  adminUser.email: airflow@airflow.com
  adminUser.password: airflow
  connections.secretKey: thisISaSECRET_1234
  connections.sqlalchemyDatabaseUri: postgresql+psycopg2://airflow:airflow@airflow-postgresql/airflow
{% if test_scenario['values']['executor'] == 'celery' %}
  connections.celeryResultBackend: db+postgresql://airflow:airflow@airflow-postgresql/airflow
  connections.celeryBrokerUrl: redis://:redis@airflow-redis-master:6379/0
{% endif %}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-cm-dag
data:
  example_trigger_target_dag.py: |
    import pendulum
    from airflow import DAG
    from airflow.decorators import task
    from airflow.operators.bash import BashOperator

    @task(task_id="run_this")
    def run_this_func(dag_run=None):
        """
        Print the payload "message" passed to the DagRun conf attribute.

        :param dag_run: The DagRun object
        :type dag_run: DagRun
        """
        print(f"Remotely received value of {dag_run.conf.get('message')} for key=message")

    with DAG(
        dag_id="example_trigger_target_dag",
        start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
        catchup=False,
        schedule=None,
        tags=['example'],
    ) as dag:
        run_this = run_this_func()

        bash_task = BashOperator(
            task_id="bash_task",
            bash_command='echo "Here is the message: $message"',
            env={'message': '{% raw %}{{ dag_run.conf.get("message") }}{% endraw %}'},
        )
  dag_factory.py: |
    from airflow import DAG
    from airflow.operators.empty import EmptyOperator
    from datetime import datetime, timedelta

    # Number of DAGs to generate
    NUM_DAGS = 1200  # Increase for more stress
    DAG_PREFIX = "stress_dag_"

    default_args = {
        'owner': 'airflow',
        'start_date': datetime(2024, 1, 1),
        'retries': 1,
        'retry_delay': timedelta(seconds=5),
    }

    def create_dag(dag_id):
        with DAG(
            dag_id=dag_id,
            default_args=default_args,
            schedule=None,
            catchup=False,
            tags=["stress_test"],
        ) as dag:
            start = EmptyOperator(task_id='start')
            end = EmptyOperator(task_id='end')
            start >> end
        return dag

    for i in range(NUM_DAGS):
        dag_id = f"{DAG_PREFIX}{i:04d}"
        globals()[dag_id] = create_dag(dag_id)

  fork.py: |
    from __future__ import annotations
    import pendulum
    from airflow.providers.cncf.kubernetes.operators.pod import (
        KubernetesPodOperator,
    )
    from airflow.sdk import DAG, task

    @task.branch()
    def should_run(**kwargs) -> str:
        """
        Determine which empty_task should be run based on if the logical date minute is even or odd.

        :param dict kwargs: Context
        :return: Id of the task to run
        """
        print(f"------------- exec dttm = {kwargs['logical_date']} and minute = {kwargs['logical_date'].minute}")
        if kwargs["logical_date"].minute % 2 == 0:
            return "branch_task_3"
        return "branch_task_4"


    with DAG(
        dag_id="branching_test",
        schedule="*/1 * * * *",
        start_date=pendulum.datetime(2025, 1, 1, tz="UTC"),
        catchup=False,
        default_args={"depends_on_past": True},
        tags=["example"],
    ) as dag:
        cond = should_run()

        task_1 = KubernetesPodOperator(
                          name=f"branch_task_1",
                          image="oci.stackable.tech/sdp/testing-tools:0.2.0-stackable0.0.0-dev",
                          cmds=["python"],
                          arguments=["-V"],
                          image_pull_policy="IfNotPresent",
                          task_id="branch_task_1",
                          do_xcom_push=True
                      )
        task_2 = KubernetesPodOperator(
                          name=f"branch_task_2",
                          image="oci.stackable.tech/sdp/testing-tools:0.2.0-stackable0.0.0-dev",
                          cmds=["python"],
                          arguments=["-V"],
                          image_pull_policy="IfNotPresent",
                          task_id="branch_task_2",
                          do_xcom_push=True
                      )
        task_3 = KubernetesPodOperator(
                          name=f"branch_task_3",
                          image="oci.stackable.tech/sdp/testing-tools:0.2.0-stackable0.0.0-dev",
                          cmds=["python"],
                          arguments=["-V"],
                          image_pull_policy="IfNotPresent",
                          task_id="branch_task_3",
                          do_xcom_push=True
                      )
        task_4 = KubernetesPodOperator(
                          name=f"branch_task_4",
                          image="oci.stackable.tech/sdp/testing-tools:0.2.0-stackable0.0.0-dev",
                          cmds=["python"],
                          arguments=["-V"],
                          image_pull_policy="IfNotPresent",
                          task_id="branch_task_4",
                          do_xcom_push=True
                      )
        task_5 = KubernetesPodOperator(
                          name=f"branch_task_5",
                          image="oci.stackable.tech/sdp/testing-tools:0.2.0-stackable0.0.0-dev",
                          cmds=["python"],
                          arguments=["-V"],
                          image_pull_policy="IfNotPresent",
                          task_id="branch_task_5",
                          do_xcom_push=True
                      )

        task_1 >> task_2 >> cond
        cond >> [task_3, task_4]
        task_3 >> task_5

---
apiVersion: airflow.stackable.tech/v1alpha1
kind: AirflowCluster
metadata:
  name: airflow
spec:
  image:
{% if test_scenario['values']['airflow-latest'].find(",") > 0 %}
    custom: "{{ test_scenario['values']['airflow-latest'].split(',')[1] }}"
    productVersion: "{{ test_scenario['values']['airflow-latest'].split(',')[0] }}"
{% else %}
    productVersion: "{{ test_scenario['values']['airflow-latest'] }}"
{% endif %}
    pullPolicy: IfNotPresent
  clusterConfig:
{% if lookup('env', 'VECTOR_AGGREGATOR') %}
    vectorAggregatorConfigMapName: vector-aggregator-discovery
{% endif %}
    credentialsSecret: test-airflow-credentials
    volumes:
      - name: test-cm-dag
        configMap:
          name: test-cm-dag
    volumeMounts:
      - name: test-cm-dag
        mountPath: /dags/example_trigger_target_dag.py
        subPath: example_trigger_target_dag.py
      - name: test-cm-dag
        mountPath: /dags/dag_factory.py
        subPath: dag_factory.py
      - name: test-cm-dag
        mountPath: /dags/fork.py
        subPath: fork.py
  webservers:
    roleConfig:
      listenerClass: external-unstable
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
        containers: &logging
          airflow:
            console:
              level: DEBUG
            file:
              level: DEBUG
            loggers:
              ROOT:
                level: DEBUG
          git-sync:
            console:
              level: DEBUG
            file:
              level: DEBUG
            loggers:
              ROOT:
                level: DEBUG
          vector:
            console:
              level: DEBUG
            file:
              level: DEBUG
            loggers:
              ROOT:
                level: DEBUG
    roleGroups:
      default:
        envOverrides:
          AIRFLOW__CORE__DAGS_FOLDER: "/dags"
        replicas: 1
{% if test_scenario['values']['executor'] == 'celery' %}
  celeryExecutors:
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
        containers: *logging
    roleGroups:
      default:
        envOverrides:
          AIRFLOW__CORE__DAGS_FOLDER: "/dags"
        replicas: 2
{% elif test_scenario['values']['executor'] == 'kubernetes' %}
  kubernetesExecutors:
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
    envOverrides:
          AIRFLOW__CORE__DAGS_FOLDER: "/dags"
{% endif %}
  schedulers:
    config:
      gracefulShutdownTimeout: 10s
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
        containers: *logging
    roleGroups:
      default:
        envOverrides:
          AIRFLOW__CORE__DAGS_FOLDER: "/dags"
        replicas: 1
